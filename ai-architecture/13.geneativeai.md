
# Generative AI & LLM Architecture  
## Enterprise Summary Notes (Study Version)

---

## 1. What Is Generative AI?

**Generative AI** is a class of AI systems that can **create new content** (text, code, images, audio) based on natural language input.

**Large Language Models (LLMs)** are Generative AI systems focused on text and code.

### Why this matters for enterprises
- Natural language becomes an **interface**
- Knowledge work is **accelerated**
- AI shifts from *predicting outcomes* to *producing content*

**Enterprise examples**
- Drafting policies, contracts, and reports  
- Internal HR / IT / Legal copilots  
- Code generation and legacy modernization  

---

## 2. Evolution of AI: Rules → RNNs → Transformers

Understanding this progression explains *why modern LLMs work the way they do*.

### 2.1 Rules-Based AI

**What it is**
- Handwritten IF–THEN logic
- Keyword matching
- No learning

**Why it failed**
- Does not scale
- Extremely brittle
- Cannot handle language variation

---

### 2.2 RNNs (Recurrent Neural Networks)

**What RNN means**
- A neural network that reads text **sequentially**
- Maintains short-term memory of previous words

**Why RNNs mattered**
- First models to understand word order
- Enabled early NLP and virtual assistants

**Limitations**
- Memory fades over long text
- Slow (one word at a time)
- Poor performance on long documents

---

### 2.3 Transformers (Modern Breakthrough)

**What a Transformer is**
- A neural network architecture that processes **all words at once**
- Uses **attention** to determine what matters most

**Why Transformers changed everything**
- Handle long context
- Faster (parallel processing)
- Cloud-scalable
- Enable text generation

---

## 3. Types of Transformers & What They Solve

### Decoder-Only Transformers
- Generate text and code  
- Examples: GPT-4, Claude, LLaMA  

### Encoder-Only Transformers
- Understand and classify text  
- Examples: BERT  

### Encoder–Decoder Transformers
- Transform text (translate, summarize)  
- Examples: T5, BART  

### Multimodal Transformers
- Handle text, images, and audio  
- Examples: GPT-4o  

---

## 4. Why Encoder vs Decoder Architecture Matters

| Role | Model Type |
|----|----|
| Understand | Encoder |
| Generate | Decoder |
| Understand + Generate | Encoder–Decoder |

**Key point:**  
- Generation requires a Decoder  
- Understanding requires an Encoder  

---

## 5. How LLMs Work (Simplified)

1. Input text is **tokenized**
2. Model predicts the **next token**
3. Repeats until response is complete
4. Randomness controlled by **temperature**

**Limitation:**  
LLMs generate *plausible*, not guaranteed correct, text.

---

## 6. Prompt Engineering

- Zero-shot: no examples  
- One-shot: one example  
- Few-shot: multiple examples  

**Pros:** cheap, fast  
**Cons:** no access to private or new data  

---

## 7. RAG (Retrieval-Augmented Generation)

### What RAG Solves
- LLMs cannot access private enterprise data
- Reduces hallucinations

### How RAG Works
1. Documents are chunked and embedded
2. Stored in a **vector database**
3. User query retrieves relevant chunks
4. Chunks are injected into the prompt
5. LLM answers using only that context

---

## 8. Prompt Engineering vs RAG

| Prompt Engineering | RAG |
|---|---|
| Static context | Dynamic data |
| No search | Semantic retrieval |
| Limited knowledge | Enterprise knowledge |

---

## 9. Typical Enterprise LLM Architecture

User → App → Vector DB → Prompt → LLM → Response

---

## 10. Model vs Service

**Model**
- Raw neural network
- You manage infrastructure

**Service (e.g., Azure OpenAI)**
- Managed, secure, scalable
- Preferred by enterprises

---

## 11. Improving LLM Results

1. Prompt engineering  
2. RAG  
3. Fine-tuning  
4. Training from scratch  

---

## 12. Responsible AI

**Risks**
- Hallucinations
- Harmful or biased content

**Mitigations**
- RAG
- Content filtering
- Human review
- Monitoring

---

## 13. Key Takeaways

- Rules are brittle
- RNNs were transitional
- Transformers enable modern AI
- Decoders generate, encoders understand
- RAG is the default enterprise pattern
- Architecture choices matter
