# Naive Bayes Classification Summary

## Introduction
Naive Bayes classification is a machine learning technique based on Bayes' theorem. It is used to classify objects, such as text documents, into two or more classes. The classifier is trained using labeled training data.

## How It Works
The naive Bayes classifier assumes that feature variables are conditionally independent given the class. This simplification allows the model to compute the probability of a class given a set of observations efficiently. In Naive Bayes, we assume that each feature (input variable) contributes independently to the probability of a certain class once we know the class label.

## Real-World Application 1: Spam Filters
A common application is spam filtering, where the class variable indicates whether a message is spam or ham (legitimate). Words in the message are treated as feature variables.

### Why "Naive"?
The model assumes that words are chosen independently based on the class, ignoring word order and dependencies. Despite this simplification, the method performs well in practice.

## Estimating Parameters
- **Prior Odds**: Assume 1:1 for spam vs. ham.
- **Word Probabilities**: Estimated from training data by counting word occurrences in spam and ham messages.

### Example Word Counts
| Word         | Spam | Ham  |
|--------------|------|------|
| million      | 156  | 98   |
| dollars      | 29   | 119  |
| adclick      | 51   | 0    |
| conferences  | 0    | 12   |
| Total Words  | 95791| 306438|

### Likelihood Ratios
| Word         | Likelihood Ratio |
|--------------|------------------|
| million      | 5.1              |
| dollars      | 0.8              |
| adclick      | 53.2             |
| conferences  | 0.3              |

## Real World Example 2: Classifying Fruits
Imagine you're trying to guess the type of fruit based on features like:
- Color
- Shape
- Size

Let’s say the fruit is an apple. The Naive Bayes assumption says:
- Given that it’s an apple, the color (red), shape (round), and size (medium) are all independent of each other.

But in reality, these features might be correlated:
- Red apples are often medium-sized and round.
- Green apples might be smaller and more oval.

So the assumption is naive because it ignores these correlations.

In this Example
- **Class**
    - The category of label we are trying to predict
    - In this case: Type of Fruit
- **Class Labels**
    - The possible values the class can take
    - Examples
        - Apple
        - Banana
- **Features**
    - The input variable or attributes used to make the prediction
    - Examples:
        - Color(eg red, yellow, green)
        - Shape(eg round, long)
        - Size(eg small, medium, long)

### Why This Assumption?
Even though the independence assumption is often not true, Naive Bayes still works surprisingly well in many cases, especially when:
- The dataset is large.
- The features are only weakly dependent.
- The model is used for classification, not for understanding relationships.

## Handling Zero Counts
Zero counts can lead to zero probabilities, which are problematic. A small lower bound (e.g., 1/100000) is used to avoid this issue.

## Conclusion
Naive Bayes is a powerful and practical classification technique. It demonstrates how probabilistic reasoning can handle uncertainty and conflicting evidence effectively.

## References
- [Naive Bayes Classification](https://course.elementsofai.com/3/3)
- [George EP Box](https://en.wikipedia.org/wiki/George_E._P._Box)

